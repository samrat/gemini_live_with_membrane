# Gemini Multimodal Live <> Membrane WebRTC

```elixir
File.cd(__DIR__)
Logger.configure(level: :info)

Mix.install([
  {:membrane_core, "~> 1.1"},
  {:membrane_webrtc_plugin, "~> 0.22.0"},
  {:membrane_opus_plugin, "~> 0.20.4"},
  {:membrane_raw_audio_parser_plugin, "~> 0.4.0"},
  {:membrane_realtimer_plugin, "~> 0.10.0"},
  {:kino_membrane, "~> 0.3.0"},
  {:websockex, "~> 0.4.3"},
  {:jason, "~> 1.4"},
  {:ex_webrtc, github: "elixir-webrtc/ex_webrtc", override: true},
  {:membrane_h26x_plugin, "~> 0.10.2"},
  {:membrane_h264_ffmpeg_plugin, "~> 0.32.5"},
  {:turbojpeg, github: "gBillal/elixir-turbojpeg", branch: "upgrade-deps"}
],
  consolidate_protocols: false)
```

## Introduction

This demo shows how to use Membrane Framework to create a simple WebRTC based app that allows you to have a conversation with Google Gemini using the [Multimodal Live API](https://ai.google.dev/api/multimodal-live).

## Playback Queue

Instead of sending Gemini's audio responses directly to the WebRTC sink, we queue it up using a Membrane filter called `PlaybackQueue`. This filter enables us to clear the queue when the user interrupts Gemini, and we no longer want to play the rest of Gemini's current response.

```elixir
defmodule PlaybackQueue.Events.Reset do
  @moduledoc """
  Event that resets `PlaybackQueue`
  After receiving this event, `Membrane.Realtimer` will behave as if it would be freshly spawned.
  """

  @derive Membrane.EventProtocol
  defstruct []
end
```

```elixir
defmodule PlaybackQueue do
  use Membrane.Filter
  require Membrane.Logger

  def_input_pad(:input, accepted_format: _any)

  def_output_pad(:output, accepted_format: _any, flow_control: :push)

  @impl true
  def handle_init(_ctx, _opts) do
    {[], %{audio_buffer: <<>>}}
  end

  @impl true
  def handle_playing(_ctx, state) do
    format = %Membrane.RawAudio{channels: 1, sample_rate: 24_000, sample_format: :s16le}
    
    Process.send_after(self(), :push_buffers, 20)
    {[stream_format: {:output, format}], state}
  end

  @impl true
  def handle_buffer(:input, buffer, _ctx, state) do
    {[], %{state | audio_buffer: state.audio_buffer <> buffer.payload}}
  end

  def handle_info(:push_buffers, _ctx, state) do
    # We send 20 millisecond chunks
    chunk_duration_ms = 20
    # Samples per second * bytes per sample * chunk duration in seconds
    chunk_byte_size = trunc(24_000 * 2 * chunk_duration_ms / 1_000)

    {chunk, new_audio_buffer} = get_chunk(state.audio_buffer, chunk_byte_size)
    Process.send_after(self(), :push_buffers, 20)

    {[
      buffer: {:output, %Membrane.Buffer{payload: chunk}}
    ], %{state | audio_buffer: new_audio_buffer}}
  end

  def handle_event(:input, %PlaybackQueue.Events.Reset{}, _ctx, state) do
    {[], %{state | audio_buffer: <<>>}}
  end

  def get_chunk(audio_buffer, size) do
    case audio_buffer do
      <<chunk::binary-size(size), rest::binary>> ->
        # If we have enough data, send it back
        {chunk, rest}
      chunk ->
        # Otherwise, send what we have, padded with silence
        silence = <<0::size(size - byte_size(chunk))-unit(8)>>
        {chunk <> silence, <<>>}
    end
  end
end
```

The Gemini Multimodal Live API requires sending and receiving audio via the WebSocket. Let's create a module responsible for handling it with `WebSockex` library.

```elixir
defmodule GeminiWebSocket do
  use WebSockex
  require Logger

  def start_link(opts) do
    api_key = System.get_env("LB_GEMINI_API_KEY")
    {:ok, _} = WebSockex.start_link(
      "wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent?key=#{api_key}",
      __MODULE__,
      %{parent: self()},
      opts
    )
  end

  @impl true
  def handle_connect(_conn, state) do
    setup_msg = Jason.encode!(%{
      setup: %{
        model: "models/gemini-2.0-flash-exp"
      }
    })
    WebSockex.cast(self(), {:send_message, {:text, setup_msg}})
    {:ok, state}
  end

  @impl true
  def handle_cast({:send_message, frame}, state) do
    {:reply, frame, state}
  end

  @impl true
  def handle_frame(frame, state) do
    send(state.parent, {:websocket_frame, frame})
    {:ok, state}
  end

  def send_frame(ws, frame), do: WebSockex.send_frame(ws, {:text, frame})
end
```

```elixir
defmodule GeminiEndpoint do
  use Membrane.Filter
  require Membrane.Logger

  def_input_pad(:audio, accepted_format: _any)
  def_input_pad(:video, accepted_format: _any)

  def_output_pad(:output, accepted_format: _any)

  def_options(websocket_opts: [])

  @impl true
  def handle_init(_ctx, opts) do
    {:ok, ws} = GeminiWebSocket.start_link(opts.websocket_opts)
    {[], %{ws: ws}}
  end

  @impl true
  def handle_buffer(:audio, buffer, _ctx, state) do
    audio = Base.encode64(buffer.payload)
    sample_rate = 16_000

    frame =
      %{
        realtime_input: %{
          media_chunks: [
            %{
              mime_type: "audio/pcm;rate=#{sample_rate}",
              data: audio
            }
          ]
        }
      }
      |> Jason.encode!()

    :ok = GeminiWebSocket.send_frame(state.ws, frame)
    {[], state}
  end

  @impl true
  def handle_buffer(:video, buffer, _ctx, state) do
    video = Base.encode64(buffer.payload)
    frame =
      %{
        realtime_input: %{
          media_chunks: [
            %{
              mime_type: "image/jpeg",
              data: video
            }
          ]
        }
      }
      |> Jason.encode!()
    :ok = GeminiWebSocket.send_frame(state.ws, frame)
    {[], state}
  end
  
  @impl true
  def handle_info({:websocket_frame, {:binary, frame}}, _ctx, state) do
    case Jason.decode!(frame) do
      %{
        "serverContent" => %{
          "modelTurn" => %{
            "parts" => [
              %{"inlineData" => %{"data" => audio_data, "mimeType" => "audio/pcm;rate=24000"}}
            ]
          }
        }
      } ->
        audio_payload = Base.decode64!(audio_data)
        {[buffer: {:output, %Membrane.Buffer{payload: audio_payload}}], state}

      %{
        "serverContent" => %{
          "interrupted" => true
        }
      } ->
        {[event: {:output, %PlaybackQueue.Events.Reset{}}], state}

      response ->
        Membrane.Logger.debug("Unhandled response: #{inspect(response)}")
        {[], state}
    end
  end
end
```

## Membrane Components

Now, let's write a Pipeline module that exchanges the media with the browser using `Membrane.WebRTC.Source` and `Sink` and with Gemini server using `GeminiEndpoint`.

Because WebRTC requires and provides audio in OPUS format and Gemini API uses raw audio, we have to spawn the proper encoder and decoder between WebRTC and Gemini elements.

```elixir
defmodule GeminiPipeline do
  use Membrane.Pipeline

  @impl true
  def handle_init(_ctx, opts) do
    # {:ok, playback_queue} = PlaybackQueue.start_link(nil)

    spec = [
      # WebRTC Source -> Gemini -> PlaybackQueue
      child(:webrtc_source, %Membrane.WebRTC.Source{
        signaling: {:websocket, port: opts[:webrtc_source_ws_port]}
      })
      |> via_out(:output, options: [kind: :audio])
      |> child(:input_opus_parser, Membrane.Opus.Parser)
      |> child(:opus_decoder, %Membrane.Opus.Decoder{sample_rate: 24_000})
      |> via_in(:audio)
      |> child(:gemini, %GeminiEndpoint{
        websocket_opts: opts[:gemini_ws_opts],
        # playback_queue_pid: playback_queue
      })
      |> child(
        :playback_queue, PlaybackQueue
      )
      |> child(:raw_audio_parser, %Membrane.RawAudioParser{overwrite_pts?: true})
      |> via_in(:input, target_queue_size: 1_000_000_000, toilet_capacity: 1_000_000_000)
      |> child(:realtimer, Membrane.Realtimer)
      |> child(:opus_encoder, Membrane.Opus.Encoder)
      |> via_in(:input, options: [kind: :audio])
      |> child(:webrtc_sink, %Membrane.WebRTC.Sink{
        tracks: [:audio],
        signaling: {:websocket, port: opts[:webrtc_sink_ws_port]}
      }),

      # Send video to Gemini
      get_child(:webrtc_source)
      |> via_out(:output, options: [kind: :video])
      |> child(:h264_parser, %Membrane.H264.Parser{})
      |> child(:h264_decoder, Membrane.H264.FFmpeg.Decoder)
      |> child(:jpeg_encoder, Turbojpeg.Filter)
      |> via_in(:video)
      |> get_child(:gemini)
    ]

    {[spec: spec], %{}}
  end
end
```

## Running the server

Now, let's start the pipeline.

```elixir
{:ok, _supervisor, pipeline} =
  Membrane.Pipeline.start_link(GeminiPipeline,
    gemini_ws_opts: [],
    webrtc_source_ws_port: 8829,
    webrtc_sink_ws_port: 8831
  )

:inets.start()

:inets.start(:httpd,
  bind_address: ~c"localhost",
  port: 8000,
  document_root: ~c"#{__DIR__}/assets",
  server_name: ~c"webrtc",
  server_root: "/tmp"
)

Process.monitor(pipeline)

KinoMembrane.pipeline_dashboard(pipeline)
```

Enter <http://localhost:8000/index.html> from the new tab of Google Chrome and start your conversation with the AI!

Transcription of AI answers will be available in the logs of the cell below.

```elixir
receive do
  {:DOWN, _ref, :process, ^pipeline, _reason} -> :ok
end
```

<!-- livebook:{"offset":9068,"stamp":{"token":"XCP.2AoasipLhKhWm3XjNBSsEy3YFzd7JIID_sU9nIk9EthKtk7dtCQhsIZkT2Ucu9z-U07MN0LlXLpDtwIhnrhnHrzv_S4I843sac6fAqpsWt1vMk-FwnyAeRTMu0jX8I1Or9hRbxmyPKxEAJw4","version":2}} -->
